{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Mundo\n"
     ]
    }
   ],
   "source": [
    "print(\"Hola Mundo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "!ls work/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"./work/dataset/product_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "+--------------+----------+--------+--------+--------+------------+\n",
      "|transaction_id|      date|category| product|quantity|       price|\n",
      "+--------------+----------+--------+--------+--------+------------+\n",
      "|             1|2024-07-01|  Widget|Widget-A|      10|        9.99|\n",
      "|             2|2024-07-01|  Gadget|Gadget-X|       5|       19.99|\n",
      "|             3|2024-07-02|  Widget|Widget-B|       7|        9.99|\n",
      "|             4|2024-07-02|  Doodad|Doodad-1|    NULL|       4.99 |\n",
      "|             5|2024-07-03|  Widget|Widget-C|       3|        9.99|\n",
      "|             6|2024-07-03|  Gadget|Gadget-Y|       8|       19.99|\n",
      "|             7|2024-07-04|  Widget|Widget-A|       2|        9.99|\n",
      "|             8|2024-07-04|  Doodad|Doodad-2|       4|not_a_number|\n",
      "|             9|2024-07-05|  Widget|Widget-B|       6|        9.99|\n",
      "|            10|2024-07-05|  Gadget|Gadget-X|       3|       19.99|\n",
      "|            11|2024-07-06|  Gadget|    NULL|       5|       19.99|\n",
      "|            12|2024-07-06|  Doodad|Doodad-3|       1|        4.99|\n",
      "|            13|2024-07-07|  Widget|Widget-C|       8|        9.99|\n",
      "|            14|2024-07-07|  Gadget|Gadget-Y|       4|       19.99|\n",
      "|            15|2024-07-08|  Widget|Widget-A|       3|        9.99|\n",
      "|            16|2024-07-08|  Doodad|Doodad-1|       2|        4.99|\n",
      "|            17|2024-07-09|  Gadget|Gadget-X|    NULL|       19.99|\n",
      "|            18|2024-07-09|  Widget|Widget-B|       5|        9.99|\n",
      "|            19|2024-07-10|  Doodad|Doodad-2|       4|        4.99|\n",
      "|            20|2024-07-10|  Gadget|Gadget-Y|       7|       19.99|\n",
      "+--------------+----------+--------+--------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_resource = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df_resource.printSchema()\n",
    "df_resource.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tData Cleaning:\n",
    "\n",
    "    •\tHandle missing values:\n",
    "\n",
    "        •\tReplace missing quantity with 0.\n",
    "\n",
    "        •\tReplace missing or invalid price values (not_a_number) with the median price for the same product category.\n",
    "\n",
    "        •\tDrop rows where both quantity and price are invalid or missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+--------+--------+-----+\n",
      "|category|transaction_id|      date| product|quantity|price|\n",
      "+--------+--------------+----------+--------+--------+-----+\n",
      "|  Widget|             1|2024-07-01|Widget-A|      10| 9.99|\n",
      "|  Gadget|             2|2024-07-01|Gadget-X|       5|19.99|\n",
      "|  Widget|             3|2024-07-02|Widget-B|       7| 9.99|\n",
      "|  Doodad|             4|2024-07-02|Doodad-1|       0| 4.99|\n",
      "|  Widget|             5|2024-07-03|Widget-C|       3| 9.99|\n",
      "|  Gadget|             6|2024-07-03|Gadget-Y|       8|19.99|\n",
      "|  Widget|             7|2024-07-04|Widget-A|       2| 9.99|\n",
      "|  Doodad|             8|2024-07-04|Doodad-2|       4| 4.99|\n",
      "|  Widget|             9|2024-07-05|Widget-B|       6| 9.99|\n",
      "|  Gadget|            10|2024-07-05|Gadget-X|       3|19.99|\n",
      "+--------+--------------+----------+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Replace missing quantity with 0.\n",
    "df = df_resource.fillna({\"quantity\": 0})\n",
    "\n",
    "# Replace missing or invalid price values (not_a_number) with the median price for the same product category.\n",
    "df = df.withColumn(\"price\", F.when(F.col(\"price\") == \"not_a_number\", None).otherwise(F.col(\"price\").cast(\"float\")))\n",
    "\n",
    "# Calculate Median by category\n",
    "filtered_for_median = df.filter(F.col(\"price\").isNotNull())\n",
    "window_spec = Window.partitionBy(\"category\")\n",
    "median_price_df = filtered_for_median.withColumn(\"median_price\", F.expr(\"percentile_approx(price, 0.5)\").over(window_spec))\n",
    "\n",
    "# Replace missing or invalid price values\n",
    "median_price_df = median_price_df.select(\"category\", \"median_price\").dropDuplicates([\"category\"])\n",
    "df = df.join(median_price_df, on=\"category\", how=\"left\")\n",
    "df = df.withColumn(\"price\", F.when(F.col(\"price\").isNull(), F.col(\"median_price\")).otherwise(F.col(\"price\")))\n",
    "\n",
    "# Drop rows where both quantity and price are invalid or missing\n",
    "df = df.filter(~(F.col(\"quantity\").isNull() & F.col(\"price\").isNull()))\n",
    "\n",
    "# Delete median_price_column\n",
    "df = df.drop(\"median_price\")\n",
    "\n",
    "# Show data clean results\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+--------+--------+-----+\n",
      "|category|transaction_id|      date| product|quantity|price|\n",
      "+--------+--------------+----------+--------+--------+-----+\n",
      "|  Doodad|             8|2024-07-04|Doodad-2|       4| 4.99|\n",
      "|  Gadget|            32|2024-07-16|Gadget-Y|       5|19.99|\n",
      "+--------+--------------+----------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transaction_ids = [4, 8, 11, 17, 27, 32, 40, 47]\n",
    "transaction_ids = [8,32]\n",
    "filtered_df = df.filter(F.col(\"transaction_id\").isin(transaction_ids))\n",
    "\n",
    "# Mostrar los resultados\n",
    "filtered_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tDerived Columns:\n",
    "\n",
    "    •\tCalculate total_sales (quantity * price) for each transaction.\n",
    "\n",
    "    •\tCreate a day_of_week column based on the date.\n",
    "\n",
    "    •\tAdd a high_volume flag: True if quantity > 10, otherwise False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+--------+--------+-----+-----------+-----------+-----------+\n",
      "|category|transaction_id|      date| product|quantity|price|total_sales|day_of_week|high_volume|\n",
      "+--------+--------------+----------+--------+--------+-----+-----------+-----------+-----------+\n",
      "|  Widget|             1|2024-07-01|Widget-A|      10| 9.99|  99.899994|     Monday|      false|\n",
      "|  Gadget|             2|2024-07-01|Gadget-X|       5|19.99|      99.95|     Monday|      false|\n",
      "|  Widget|             3|2024-07-02|Widget-B|       7| 9.99|      69.93|    Tuesday|      false|\n",
      "|  Doodad|             4|2024-07-02|Doodad-1|       0| 4.99|        0.0|    Tuesday|      false|\n",
      "|  Widget|             5|2024-07-03|Widget-C|       3| 9.99|      29.97|  Wednesday|      false|\n",
      "|  Gadget|             6|2024-07-03|Gadget-Y|       8|19.99|     159.92|  Wednesday|      false|\n",
      "|  Widget|             7|2024-07-04|Widget-A|       2| 9.99|      19.98|   Thursday|      false|\n",
      "|  Doodad|             8|2024-07-04|Doodad-2|       4| 4.99|      19.96|   Thursday|      false|\n",
      "|  Widget|             9|2024-07-05|Widget-B|       6| 9.99|      59.94|     Friday|      false|\n",
      "|  Gadget|            10|2024-07-05|Gadget-X|       3|19.99|      59.97|     Friday|      false|\n",
      "+--------+--------------+----------+--------+--------+-----+-----------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total_sales (quantity * price) for each transaction\n",
    "df = df.withColumn(\"total_sales\", F.col(\"quantity\") * F.col(\"price\"))\n",
    "\n",
    "# Create a day_of_week column based on the date.\n",
    "df = df.withColumn(\"day_of_week\", F.date_format(F.col(\"date\"), \"EEEE\"))\n",
    "\n",
    "# Add a high_volume flag: True if quantity > 10, otherwise False.\n",
    "df = df.withColumn(\"high_volume\", F.when(F.col(\"quantity\") > 10, True).otherwise(False))\n",
    "\n",
    "# Show dervied columns results\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----+-------+--------+-----+-----------+-----------+-----------+\n",
      "|category|transaction_id|date|product|quantity|price|total_sales|day_of_week|high_volume|\n",
      "+--------+--------------+----+-------+--------+-----+-----------+-----------+-----------+\n",
      "+--------+--------------+----+-------+--------+-----+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(F.col(\"high_volume\")==True)\n",
    "\n",
    "# Mostrar los resultados\n",
    "filtered_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tComplex Transformations:\n",
    "\n",
    "    •\tGroup data by category and calculate:\n",
    "\n",
    "        •\tAverage price per product in the category.\n",
    "\n",
    "        •\tTotal revenue for each category.\n",
    "\n",
    "        •\tDay with highest sales for the category.\n",
    "\n",
    "    •\tIdentify outliers in the data:\n",
    "\n",
    "        •\tTransactions where quantity is more than 2 standard deviations from the category mean.\n",
    "\n",
    "        •\tMark these rows with an outlier flag.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+--------+--------+-----+-----------+-----------+-----------+------------+\n",
      "|category|transaction_id|      date| product|quantity|price|total_sales|day_of_week|high_volume|outlier_flag|\n",
      "+--------+--------------+----------+--------+--------+-----+-----------+-----------+-----------+------------+\n",
      "|  Widget|             1|2024-07-01|Widget-A|      10| 9.99|  99.899994|     Monday|      false|       false|\n",
      "|  Gadget|             2|2024-07-01|Gadget-X|       5|19.99|      99.95|     Monday|      false|       false|\n",
      "|  Widget|             3|2024-07-02|Widget-B|       7| 9.99|      69.93|    Tuesday|      false|       false|\n",
      "|  Doodad|             4|2024-07-02|Doodad-1|       0| 4.99|        0.0|    Tuesday|      false|       false|\n",
      "|  Widget|             5|2024-07-03|Widget-C|       3| 9.99|      29.97|  Wednesday|      false|       false|\n",
      "|  Gadget|             6|2024-07-03|Gadget-Y|       8|19.99|     159.92|  Wednesday|      false|       false|\n",
      "|  Widget|             7|2024-07-04|Widget-A|       2| 9.99|      19.98|   Thursday|      false|       false|\n",
      "|  Doodad|             8|2024-07-04|Doodad-2|       4| 4.99|      19.96|   Thursday|      false|       false|\n",
      "|  Widget|             9|2024-07-05|Widget-B|       6| 9.99|      59.94|     Friday|      false|       false|\n",
      "|  Gadget|            10|2024-07-05|Gadget-X|       3|19.99|      59.97|     Friday|      false|       false|\n",
      "+--------+--------------+----------+--------+--------+-----+-----------+-----------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Group data by category and calculate\n",
    "\n",
    "# Average price per product in the category.\n",
    "df_category_avg_price = df.groupBy(\"category\", \"product\").agg(\n",
    "    F.avg(\"price\").alias(\"avg_price\")\n",
    ")\n",
    "\n",
    "# Total revenue for each category.\n",
    "df_category_revenue = df.groupBy(\"category\").agg(\n",
    "    F.sum(\"total_sales\").alias(\"total_revenue\")\n",
    ")\n",
    "\n",
    "# Day with highest sales for the category.\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(F.desc(\"total_sales\"))\n",
    "df_sales_ranked = df.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "df_day_highest_sales = df_sales_ranked.filter(F.col(\"rank\") == 1).select(\"category\", \"day_of_week\").dropDuplicates([\"category\"])\n",
    "\n",
    "### Identify outliers in the data:\n",
    "\n",
    "# Transactions where quantity is more than 2 standard deviations from the category mean.\n",
    "df_stats = df.groupBy(\"category\").agg(\n",
    "    F.mean(\"quantity\").alias(\"mean_quantity\"),\n",
    "    F.stddev(\"quantity\").alias(\"stddev_quantity\")\n",
    ")\n",
    "\n",
    "# Mark these rows with an outlier flag.\n",
    "df_stats = df_stats.dropDuplicates([\"category\"])\n",
    "df = df.join(df_stats, on=\"category\", how=\"left\")\n",
    "df = df.withColumn(\"outlier_flag\", F.when(\n",
    "    (F.col(\"quantity\") > F.col(\"mean_quantity\") + 2 * F.col(\"stddev_quantity\")) |\n",
    "    (F.col(\"quantity\") < F.col(\"mean_quantity\") - 2 * F.col(\"stddev_quantity\")),\n",
    "    True\n",
    ").otherwise(False))\n",
    "\n",
    "# Joins results\n",
    "df = df.drop(\"mean_quantity\", \"stddev_quantity\")\n",
    "\n",
    "# Show Resultds\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----+-------+--------+-----+-----------+-----------+-----------+------------+\n",
      "|category|transaction_id|date|product|quantity|price|total_sales|day_of_week|high_volume|outlier_flag|\n",
      "+--------+--------------+----+-------+--------+-----+-----------+-----------+-----------+------------+\n",
      "+--------+--------------+----+-------+--------+-----+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(F.col(\"outlier_flag\")==True)\n",
    "\n",
    "# Mostrar los resultados\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tStorage:\n",
    "\n",
    "    •\tStore the cleaned and processed data in a SQLite database with separate tables for:\n",
    "\n",
    "        •\tTransactions\n",
    "\n",
    "        •\tAggregated metrics by category\n",
    "\n",
    "        •\tOutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos SQLite actualizada correctamente en el volumen compartido.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Path to DB SQLite with the shared volumne of docker\n",
    "db_path = \"/home/jovyan/work/data/sales_dashboard.db\"\n",
    "\n",
    "# Check DB if null, create\n",
    "if not os.path.exists(db_path):\n",
    "    open(db_path, 'w').close()  # Make empty file for SQLite\n",
    "\n",
    "# Connecto to SQLite\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Delete tables if any exis\n",
    "cursor.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS transactions;\n",
    "DROP TABLE IF EXISTS aggregated_metrics;\n",
    "DROP TABLE IF EXISTS outliers;\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Save Transactions in SQLite\n",
    "df_transactions = df.select(\n",
    "    \"transaction_id\", \"date\", \"category\", \"product\", \"quantity\", \"price\",\n",
    "    \"total_sales\", \"day_of_week\", \"high_volume\"\n",
    ")\n",
    "df_transactions.toPandas().to_sql(\"transactions\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Save Aggregated Metrics\n",
    "df_aggregated_metrics = df_category_avg_price \\\n",
    "    .join(df_category_revenue, on=\"category\", how=\"inner\") \\\n",
    "    .join(df_day_highest_sales, on=\"category\", how=\"left\")\n",
    "\n",
    "df_aggregated_metrics.toPandas().to_sql(\"aggregated_metrics\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Save Guardar Outliers (Only transactiones like True)\n",
    "#df_outliers = df.filter(F.col(\"outlier_flag\") == True).select(\"transaction_id\", \"category\", \"product\", \"quantity\", \"price\", \"total_sales\", \"outlier_flag\")\n",
    "df_outliers = df.select(\"transaction_id\", \"category\", \"product\", \"quantity\", \"price\", \"total_sales\", \"outlier_flag\")\n",
    "df_outliers.toPandas().to_sql(\"outliers\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Close connection to SQLite\n",
    "conn.close()\n",
    "\n",
    "print(\"Base de datos SQLite actualizada correctamente en el volumen compartido.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
