# **Sales Dashboard API & Data Pipeline**
ğŸš€ **Procesamiento de datos de ventas con ETL y API REST usando FastAPI, SQLite y Spark.**

Este proyecto procesa datos de ventas desde un archivo CSV, los limpia, los almacena en SQLite y expone una API REST con FastAPI.

---

## ** 1. Estructura del Proyecto**
```
/Solution
â”‚â”€â”€ /API                # CÃ³digo del backend (FastAPI)
â”‚   â”‚â”€â”€ api.py          # API principal que une todas las versiones
â”‚   â”‚â”€â”€ api_simple.py   # API bÃ¡sica sin filtros
â”‚   â”‚â”€â”€ api_filter.py   # API con filtros y paginaciÃ³n
â”‚â”€â”€ /Dataset
â”‚   â”‚â”€â”€ product_dataset.csv  # Dataset con datos de ventas
â”‚â”€â”€ /Scripts
â”‚   â”‚â”€â”€ Solution_Spark.ipynb  # Notebook de procesamiento de datos
â”‚   â”‚â”€â”€ SQL_fetching_aggregated.ipynb  # Consultas SQL y validaciÃ³n de datos
â”‚   â”‚â”€â”€ /SQL
â”‚   â”‚   â”œâ”€â”€ aggregated_metrics.sql
â”‚   â”‚   â”œâ”€â”€ outliers.sql
â”‚   â”‚   â”œâ”€â”€ total_sales_day.sql
â”‚   â”‚   â”œâ”€â”€ total_sales_product.sql
/Stack
â”‚â”€â”€ docker-compose.yaml  # OrquestaciÃ³n de contenedores
â”‚â”€â”€ /FastAPI
â”‚   â”‚â”€â”€ Dockerfile  # ConfiguraciÃ³n del backend en FastAPI
â”‚   â”‚â”€â”€ requirements.txt  # Dependencias de la API
â”‚â”€â”€ /Spark
â”‚   â”‚â”€â”€ Dockerfile  # ConfiguraciÃ³n de Spark
â”‚   â”‚â”€â”€ requirements.txt
â”‚â”€â”€ /SQLite
â”‚   â”‚â”€â”€ Dockerfile  # ConfiguraciÃ³n de SQLite en Docker
â”‚   â”‚â”€â”€ /data
â”‚   â”‚   â”œâ”€â”€ sales_dashboard.db  # Base de datos SQLite
```

---

## ** 2. CÃ³mo Levantar el Proyecto**
### **ğŸ”¹ 1ï¸âƒ£ Clonar el repositorio**
```bash
git clone https://github.com/tuusuario/sales-dashboard.git
cd sales-dashboard/Stack
```

### **ğŸ”¹ 2ï¸âƒ£ Construir y levantar los contenedores con Docker**
```bash
docker-compose up --build -d
```
 Esto levanta los servicios de **Spark, SQLite y FastAPI**.

### **ğŸ”¹ 3ï¸âƒ£ Ejecutar el procesamiento de datos en Spark**
Abrir Jupyter Notebook desde Spark en `http://localhost:8888` y ejecutar:
- **`Solution/Scripts/Solution_Spark.ipynb`**

Esto procesarÃ¡ los datos y los guardarÃ¡ en SQLite.

### **ğŸ”¹ 4ï¸âƒ£ Probar la API en Swagger UI**
Acceder a **Swagger UI** en:
```
http://localhost:8000/docs
```

<a href="solution/images/A1.JPG" target="_blank">
    <img src="solution/images/A1.JPG" alt="Docs1" width="400px">
</a>


ğŸ”¹ **Redoc UI (alternativa)**:  
```
http://localhost:8000/redoc
```
<a href="solution/images/A2.JPG" target="_blank">
    <img src="solution/images/A2.JPG" alt="Docs2" width="400px">
</a>

---

## ** 3. API Endpoints**
### **ğŸ”¹ Version 1 (`/v1` - API Simple)**
| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| `GET` | `/v1/sales/product` | Devuelve ventas por producto |
| `GET` | `/v1/sales/day` | Devuelve ventas por dÃ­a |
| `GET` | `/v1/sales/category` | Devuelve mÃ©tricas agregadas por categorÃ­a |
| `GET` | `/v1/sales/outliers` | Devuelve transacciones atÃ­picas |

### **ğŸ”¹ Version 2 (`/v2` - API con Filtros y PaginaciÃ³n)**
| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| `POST` | `/v2/sales/product` | Filtra ventas por `product`, `category` |
| `POST` | `/v2/sales/day` | Filtra ventas por `start_date`, `end_date` |
| `POST` | `/v2/sales/category` | Devuelve mÃ©tricas con paginaciÃ³n |
| `POST` | `/v2/sales/outliers` | Devuelve outliers con paginaciÃ³n |

---

## ** 4. Ejemplos de Uso con Postman**
### **ğŸ”¹ 1ï¸âƒ£ Obtener ventas por producto con filtros**
 **Cuerpo JSON (Body -> raw -> JSON)**
```json
{
    "product": "Gadget-X",
    "category": "Gadget",
    "page": 1,
    "page_size": 5
}
```
 **Ejecutar en cURL**
```bash
curl -X 'POST' 'http://localhost:8000/v2/sales/product' \
-H 'Content-Type: application/json' \
-d '{
    "product": "Gadget-X",
    "category": "Gadget",
    "page": 1,
    "page_size": 5
}'
```

### **ğŸ”¹ 2ï¸âƒ£ Obtener ventas por dÃ­a en un rango de fechas**
```json
{
    "start_date": "2024-07-01",
    "end_date": "2024-07-10",
    "page": 2,
    "page_size": 10
}
```

---

## ** 5. TecnologÃ­as Utilizadas**
ğŸ”¹ **FastAPI** â†’ Para exponer la API REST.  
ğŸ”¹ **Spark + Pandas** â†’ Para procesar los datos del CSV.  
ğŸ”¹ **SQLite** â†’ Para almacenar datos procesados y servir como base de datos.  
ğŸ”¹ **Docker + Docker Compose** â†’ Para contenerizar y orquestar la aplicaciÃ³n.  

---

## ** 6. CÃ³mo Apagar los Contenedores**
Para detener los servicios, usa:
```bash
docker-compose down
```
Esto detendrÃ¡ y eliminarÃ¡ los contenedores sin borrar los datos almacenados en SQLite.

---

ğŸ”¥ **Â¡Listo! 